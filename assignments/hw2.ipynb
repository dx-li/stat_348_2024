{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2:** Priors, regularization, and shrinkage\n",
    "STATS348, UChicago, Spring 2024\n",
    "\n",
    "----------------\n",
    "**Your name here:**\n",
    "\n",
    "----------------\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/aschein/stat_348_2024/blob/main/assignments/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "The purpose of this homework is apply the ideas from lectures 3 and 4 on regularizers, priors, and shrinkage.\n",
    "\n",
    "Please fill out your answers in the provided spaces below. When you are finished, export the notebook as a PDF, making sure that all of your solutions are clearly visible.\n",
    "\n",
    "Assignment is due **Saturday April 6, 11:59pm** on GradeScope.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 1:** Regularization and Priors\n",
    "              \n",
    "Consider a standard regression setting with fixed design $X\\in\\mathbb{R}^{n\\times p}$ and\n",
    "$$\n",
    "    y=X\\beta+\\varepsilon\n",
    "$$\n",
    "where $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2 I_p)$ for variance $\\sigma^2$ considered known and fixed.\n",
    "\n",
    "The **elastic net criterion** is a regularized loss function defined as $\\ell_\\mathrm{EN}(\\lambda_1,\\lambda_2,\\beta)=\\|y-X\\beta\\|_2^2 + \\lambda_1\\|\\beta\\|_1+\\lambda_2\\|\\beta\\|_2^2$, and the **elastic net estimator** is defined as its minimizer:\n",
    "$$\n",
    "\\widehat{\\beta}^\\textrm{EN}(\\lambda_1,\\lambda_2):=\\underset{\\beta}{\\textrm{argmin}} \\,\\, \\ell_\\mathrm{EN}(\\lambda_1,\\lambda_2,\\beta)\n",
    "$$\n",
    "In lecture, we saw a correspondence between Ridge and LASSO estimators with MAP estimates under normal and Laplace priors, respectively. \n",
    "\n",
    "----\n",
    "- **1a)** Provide the form **up to proportionality** of a prior density $P(\\beta)$ on the regression coefficients such that the MAP estimate under $P(\\beta)$ corresponds to $\\widehat{\\beta}^\\mathrm{EN}(\\lambda_1,\\lambda_2)$. In the space below, please state $P(\\beta)$ clearly, and provide a brief justification. \n",
    "\n",
    "$$P(\\beta) \\propto_{\\beta} \\textrm{Your answer here}$$\n",
    "\n",
    "[Your justification here]\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1b)** Consider a dataset where the input feature $x$ is generated from a uniform distribution over the interval $[-3,3]$, and the corresponding target $y$ is generated as\n",
    "$$y_i=2x_i^3 - x_i^2 + \\varepsilon_i$$\n",
    "where $\\varepsilon_i\\sim\\mathcal{N}(0,10)$ is a noise term (with _variance_ 10). Use the code block below to simulate a dataset of $n=20$ data points in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now consider the following model of the data using a 5th degree polynomial regression. \n",
    "$$y_i=\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\ldots + \\beta_5 x_i^5 + \\varepsilon_i$$\n",
    "where the noise is assumed $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ (with _variance_ 1).\n",
    "- **1c)**  Estimate the coefficients using maximum likelihood as $\\widehat{\\beta}^\\mathrm{MLE}$. In the code block below, do this programmatically using scikit-learn's [PolynomialFeatures preprocessor](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). (You may have to import other methods/libraries as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now, consider a prior of $\\beta\\sim\\mathcal{N}(0,\\sigma_0^2)$ on the regression coefficients. \n",
    "\n",
    "- **1d)**: Provide the form of the MAP solution  $\\widehat{\\beta}^{\\textrm{MAP}}(\\sigma_0^2)$ below:\n",
    "\n",
    "$$\\widehat{\\beta}^{\\textrm{MAP}}(\\sigma_0^2) = $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **1e)** In the code block below, implement the MAP estimator for a given $\\sigma_0^2$ (using any methods or libraries you like):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_estimate(y, X, sigma0):\n",
    "    # Your code here\n",
    "    return beta_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- **1f)** Fit the MAP estimate to the synthetic data you generated above, experimenting with different values for $\\sigma_0^2$. Find a value that seems \"too large\", a value that seems \"too small\", and a value that seems \"just right\" based on plotting and inspecting the learned regression functions. (These judgements are subjective, we are not expecting specific values, just reasonable ones.)  Once you have selected three values, display clearly in a single plot the following:\n",
    "\n",
    "    - The dataset $(x_1,y_1),\\ldots,(x_{20},y_{20})$\n",
    "\n",
    "    - The estimated regression function using using $\\widehat{\\beta}_\\mathrm{MLE}$\n",
    " \n",
    "    - The three estimated regression functions $\\widehat{\\beta}^{\\textrm{MAP}}(\\sigma_0^2)$ for the selected three values of $\\sigma_0^2$\n",
    "\n",
    "    Make sure your plot includes a legend that clearly indicates the different functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "- **1g)** Discuss your findings plot above.\n",
    "\n",
    "    - How does the MLE compare to the three different MAP solutions?\n",
    "    \n",
    "    - Why did you select the three values of $\\sigma_0^2$ that you did? \n",
    "\n",
    "\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## **Problem 2:** Estimating SEPs with binomial trials\n",
    "\n",
    "#### **Setting**\n",
    "\n",
    "It is May 1968 and the USS _Scorpion_ has just disappeared somewhere in the Atlantic Ocean, likely off the coast of Spain. You are the lone statistician on board the USS _Mizar_, which has been dispatched to find the missing submarine. Your job is to guide the search as best you can, given the data at your disposal.\n",
    "\n",
    "#### **Search effectiveness probability (SEP)**\n",
    "As we saw in lecture, an important component of our decision problem is the _search effectiveness probability_ of each search cell $k$\n",
    "\n",
    "\\begin{equation}\n",
    "q_k = P(\\textrm{finding the sub in $k$} \\mid \\textrm{sub is in $k$ and the divers search $k$})\n",
    "\\end{equation}\n",
    "\n",
    "#### **Binomial trials**\n",
    "To collect data that we can use to estimate these SEPs, our divers have been running trials to see if they can recover large objects thrown overboard in each cell $k$. Define the following quantity:\n",
    "\\begin{align}\n",
    "y_k &\\in \\{0,\\dots, n_k\\}&\\textrm{ the number of successful trials in $k$}\n",
    "\\end{align}\n",
    "\n",
    "where $n_k$ is the total number of trials in $k$.\n",
    "We will assume the following likelihood for $y_k$ given the SEP $q_k$:\n",
    "\\begin{align}\n",
    "y_k &\\stackrel{\\textrm{ind.}}{\\sim} \\textrm{Binom}(n_k,\\, q_k) \\textrm{ for cell } k = 1\\dots K\n",
    "\\end{align}\n",
    "\n",
    "#### **Beta prior**\n",
    "Now further assume the following prior for the SEPs:\n",
    "\\begin{align}\n",
    "q_k &\\stackrel{\\textrm{iid}}{\\sim} \\textrm{Beta}(a_0,\\,b_0) \\textrm{ for cell } k = 1\\dots K\n",
    "\\end{align}\n",
    "where $a_0$ and $b_0$ are the Beta prior's shape parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- **2a):** Provide an analytic expression (i.e., without any integrals) for the negative log marginal likelihood $-\\log P(\\boldsymbol{y}_{1:K} \\mid \\boldsymbol{n}_{1:K}, a_0,\\,b_0\\,)$ where $\\boldsymbol{y}_{1:K}\\equiv (y_{1},\\dots,y_{K})$ and $\\boldsymbol{n}_{1:K}\\equiv (n_{1},\\dots,n_{K})$ are the data across all cells. Provide your answer below, along with a brief justification.\n",
    "\n",
    "\\begin{equation}\n",
    "-\\log P(\\boldsymbol{y}_{1:K} \\mid \\boldsymbol{n}_{1:K}, a_0,\\,b_0\\,) = \\textrm{ Your answer here }\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- **2b):** In the code cell below, implement a function that takes in two arrays for $\\boldsymbol{y}_{1:K}$ and $\\boldsymbol{n}_{1:K}$, along with a value of the parameters $(a_0, b_0)$, and computes the negative marginal log likelihood. We recommend you use functions in the `numpy` and/or `scipy` Python libraries, but you may use any other libraries if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st    # for stats-related methods\n",
    "import scipy.special as sp  # for special functions (e.g., gammaln)\n",
    "\n",
    "def neg_log_marginal_likelihood(params, y, n):\n",
    "    \"\"\" Calculate the negative log-marginal likelihood for the beta-binomial model.\n",
    "    \n",
    "    Args:\n",
    "        params (tuple): the parameters of the marginal likelihood (a0, b0)\n",
    "        y (array): the number of successes for each trial\n",
    "        n (array): the number of trials\n",
    "    \n",
    "    Returns:\n",
    "        float: the negative log-marginal likelihood\n",
    "    \"\"\"\n",
    "    a0, b0 = params\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    return neg_log_marginal_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- **2c):** Now, in the code cell below, implement a method for fitting the parameters $(a_0, b_0)$ which relies on your implementation of the negative log marginal likelihood. We recommend using `scipy.optimize.minimize` (make sure to read [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) and to experiment with different settings). You are allowed to use other methods and libraries, if you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def fit_marginal_likelihood(y, n):\n",
    "    \"\"\" Fit the parameters of the marginal likelihood to the data.\n",
    "\n",
    "    Args:\n",
    "        y (array): the number of successes for each trial\n",
    "        n (array): the number of trials\n",
    "\n",
    "        If your function requires other input arguments, include a description here.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: the MLE for a0 and b0\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "    return a0_mle, b0_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- **2d):** Use your method to fit $a_0$ and $b_0$ to the trial data. In the cell below, load in the trial data and call your method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the beta-binomial trial data\n",
    "import pandas as pd\n",
    "df_trials = pd.read_csv('binomial_trials.csv')\n",
    "y = df_trials['n_successes'].values\n",
    "n = df_trials['n_trials'].values\n",
    "\n",
    "# if your fit_marginal_likelihood function takes extra args, modify this code to pass them in here.\n",
    "a0_mle, b0_mle = fit_marginal_likelihood(y, n)\n",
    "\n",
    "print(a0_mle, b0_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- **2e):** This is an empirical Bayes procedure that can be loosely understood as \"fitting the prior\". In the code cell below, create a plot that visualizes the the PDF of the \"fitted\" Beta prior---i.e., $\\textrm{Beta}(q;\\, \\widehat{a}_0\\,\\widehat{b}_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **2f)**: In the code cell below, use your fitted parameters $(\\widehat{a}_0, \\widehat{b}_0)$ to compute the posterior means of all $K$ SEPs $$\\widehat{q}^{\\textrm{post-mean}}_k = \\mathbb{E}[q_k \\mid y_k, n_k, \\widehat{a}_0, \\widehat{b}_0]$$\n",
    "    and compare them to the maximum likelihood estimates $$\\widehat{q}^{\\textrm{MLE}}_k = \\underset{q_k}{\\textrm{argmax}}\\, P(y_k \\mid n_k, q_k)$$.\n",
    "    More specifically:\n",
    "    \n",
    "    - Compute the posterior means.\n",
    "\n",
    "    - Compute the maximum likelihood estimates.\n",
    "    \n",
    "    - Generate a scatter plot where each $(x,y)$ point is a pair $(\\widehat{q}^{\\textrm{MLE}}_k, \\widehat{q}^{\\textrm{post-mean}}_k)$ for all $k=1\\dots K$. For reference, also include the line $x=y$, and make sure the $x$- and $y$-axis both range over the full set of possible values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- **2f)**: Discuss the plot you just generated.\n",
    "\n",
    "    - What is the relationship between the maximum likelihood estimates and the posterior means?\n",
    "\n",
    "    - Why does this make sense based on your understanding of the procedure you have implemented?\n",
    "\n",
    "    - Comment on any other observations or insights.\n",
    "\n",
    "Your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
